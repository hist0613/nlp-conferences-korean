# Korean Three-Line Summarizations of Papers 2427-2456 in Proceedings of the 6th BlackboxNLP Workshop: Analyzing and Interpreting Neural Networks for NLP
###### Proceedings of the 6th BlackboxNLP Workshop: Analyzing and Interpreting Neural Networks for NLP (https://aclanthology.org/2023.blackboxnlp-1.0/)
- Anthology ID: 2023.blackboxnlp-1.0 
- Volume: Proceedings of the 6th BlackboxNLP Workshop: Analyzing and Interpreting Neural Networks for NLP 
- Authors:  
- Summary: 
    요약문을 생성할 수 없습니다.

###### Knowledge-Grounded Natural Language Recommendation Explanation (https://aclanthology.org/2023.blackboxnlp-1.1/)
- Anthology ID: 2023.blackboxnlp-1.1 
- Volume: Proceedings of the 6th BlackboxNLP Workshop: Analyzing and Interpreting Neural Networks for NLP 
- Authors: Yonatan Belinkov | Sophie Hao | Jaap Jumelet | Najoung Kim | Arya McCarthy | Hosein Mohebbi 
- Summary: 
    1. 추천 시스템의 의사 결정에 대한 설명은 사용자가 시스템에 대한 신뢰를 높이는 데 도움이 될 수 있으며, 최근 연구는 사람이 읽기 쉬운 형식으로 자연어 설명을 생성하는 데 집중하고 있다. 
    2. 이 논문에서는 사용자가 구매한 기록을 기반으로, 사용자의 선호도를 고려하면서 아이템에 대한 객관적인 설명을 생성하는 방법을 제안한다. 
    3. 실험 결과에서 제안하는 방법이 이전에 제안된 최신 모델들보다 자연어 설명 가능 추천 메트릭에서 뛰어난 성능을 보인다는 것을 보여준다.

###### Emergent Linear Representations in World Models of Self-Supervised Sequence Models (https://aclanthology.org/2023.blackboxnlp-1.2/)
- Anthology ID: 2023.blackboxnlp-1.2 
- Volume: Proceedings of the 6th BlackboxNLP Workshop: Analyzing and Interpreting Neural Networks for NLP 
- Authors: Yonatan Belinkov | Sophie Hao | Jaap Jumelet | Najoung Kim | Arya McCarthy | Hosein Mohebbi 
- Summary: 
    1. 시퀀스 모델은 어떻게 의사 결정 과정을 표현하는가? 선행 연구는 오셀로를 두는 신경망이 보드 상태의 비선형 모델을 학습했다고 제안하였다. 
    2. 이 연구에서는 보드 상태의 관련된 선형 표현을 제시하고, "내 색깔" 대비 "상대의 색깔"에 대한 탐사(probing)가 모델의 내부 상태를 해석하는 간단하면서도 강력한 방법일 수 있다는 것을 보여준다. 
    3. 이러한 내부 표현의 정확한 이해는 단순한 벡터 산술로 모델의 동작을 제어하는 것을 가능하게 한다. 이러한 선형 표현은 해석 가능성에 대한 중요한 진전을 이룰 수 있으며, 이를 검증하기 위해 세계 모델이 어떻게 계산되는지 더 탐구한다.

###### Explaining Data Patterns in Natural Language with Language Models (https://aclanthology.org/2023.blackboxnlp-1.3/)
- Anthology ID: 2023.blackboxnlp-1.3 
- Volume: Proceedings of the 6th BlackboxNLP Workshop: Analyzing and Interpreting Neural Networks for NLP 
- Authors: Yonatan Belinkov | Sophie Hao | Jaap Jumelet | Najoung Kim | Arya McCarthy | Hosein Mohebbi 
- Summary: 
    1. 대규모 언어 모델 (LLM)은 복잡한 작업을 수행하기 위해 자연어를 잘 활용할 수 있는 능력을 보여주고 있다. 이 능력을 활용하여 데이터에서 패턴을 찾고 설명할 수 있는지 알아보고자 한다. 
    2. 기존의 데이터 예시와 사전 훈련된 LLM을 사용하여 해석 가능한 자연어 문자열을 생성하는 Interpretable Autoprompting (iPrompt)을 적용한다. 
    3. 여러 데이터셋에 대한 실험에서 iPrompt는 인간이 이해할 수 있는 데이터셋 설명을 정확하게 찾아내어 유의미한 인사이트를 제공하며, 상대적으로 작은 모델 (예: 60억 개의 매개변수)을 사용하여 비교적 효율적으로 작동한다는 것을 보여준다. 또한, 과학적인 데이터셋에 대한 실험은 iPrompt가 과학적 발견에 도움이 될 수 있는 잠재력을 보여준다.

###### Probing Quantifier Comprehension in Large Language Models: Another Example of Inverse Scaling (https://aclanthology.org/2023.blackboxnlp-1.4/)
- Anthology ID: 2023.blackboxnlp-1.4 
- Volume: Proceedings of the 6th BlackboxNLP Workshop: Analyzing and Interpreting Neural Networks for NLP 
- Authors: Yonatan Belinkov | Sophie Hao | Jaap Jumelet | Najoung Kim | Arya McCarthy | Hosein Mohebbi 
- Summary: 
    1. 크기가 커짐에 따라 대형 언어 모델(Large Language Models, LLMs)은 언어 이해 작업에서 점점 더 뛰어난 성능을 보이지만, 부정이나 양자 이해와 같은 간단한 언어 테스트에서는 여전히 실패한다. 
    2. 이 논문에서는 LLMs의 양자 이해력을 측정하기 위한 대안적인 방법을 제안하고, LLMs가 크기가 커질수록 "조금(type)"과 "대부분(type)" 양자의 의미 차이를 더 잘 이해한다는 것을 보여준다. 
    3. 그러나 크기가 커질수록 LLMs의 대부분 유형 양자의 이해력은 인간의 심리 언어학 실험 및 이전 연구와 달리 나빠진다는 역방향 scaling 관찰도 한다. 이는 LLMs의 양자 이해능력을 평가하는 데 있어 가능한 원인과 관련성을 검토한다.

###### Disentangling the Linguistic Competence of Privacy-Preserving BERT (https://aclanthology.org/2023.blackboxnlp-1.5/)
- Anthology ID: 2023.blackboxnlp-1.5 
- Volume: Proceedings of the 6th BlackboxNLP Workshop: Analyzing and Interpreting Neural Networks for NLP 
- Authors: Yonatan Belinkov | Sophie Hao | Jaap Jumelet | Najoung Kim | Arya McCarthy | Hosein Mohebbi 
- Summary: 
    1. 텍스트-텍스트 개인 정보 보호 과제에는 차별화된 프라이버시 (DP)가 필요하다. 그러나 텍스트-텍스트 개인 정보 보호는 변형된 텍스트로 훈련된 언어 모델의 성능을 저하시킴으로써 알려져 있다.
    2. 우리는 변형된 전체 텍스트에 대해서 BERT로부터 추출된 내부 표현에 대한 해석 기법을 적용하여, 텍스트-텍스트 개인 정보 보호가 어떻게 언어적 레벨에서 왜곡을 초래하는지 파악하고자 한다.
    3. 실험 결과로부터 나타나는 대표적 유사성 분석을 통해 내부 표현의 전반적 유사성이 상당히 감소하는 것을 확인할 수 있다. 이러한 불일치를 풀기 위해 탐사 작업을 사용하면, 우리는 텍스트-텍스트 개인 정보 보호가 여러 형식에 걸쳐 언어 역량에 영향을 미침을 발견하며, 단어의 지역적 특성을 인코딩하는 데 문제가 있으며 단어 구간 사이의 문맥적 관계를 인코딩하지 못한다는 것을 알 수 있다.

###### “Honey, Tell Me What’s Wrong”, Global Explanation of Textual Discriminative Models through Cooperative Generation (https://aclanthology.org/2023.blackboxnlp-1.6/)
- Anthology ID: 2023.blackboxnlp-1.6 
- Volume: Proceedings of the 6th BlackboxNLP Workshop: Analyzing and Interpreting Neural Networks for NLP 
- Authors: Yonatan Belinkov | Sophie Hao | Jaap Jumelet | Najoung Kim | Arya McCarthy | Hosein Mohebbi 
- Summary: 
    1. 복잡한 머신러닝에 대한 보편화된 설명 알고리즘의 중요성이 높아져서, 이 논문에서는 모델에 상관없는 설명 방법을 제안한다. 
    2. Therapy라는 방법은 텍스트에 적용되는 첫 번째 전역적이고 모델에 상관없는 설명 방법이며, 초기 데이터를 필요로 하지 않는다. 
    3. 실험 결과, 초기 데이터 없이도 설명을 생성할 수 있으며, 기존 방법들보다도 더 좋은 결과를 제공한다.

###### Self-Consistency of Large Language Models under Ambiguity (https://aclanthology.org/2023.blackboxnlp-1.7/)
- Anthology ID: 2023.blackboxnlp-1.7 
- Volume: Proceedings of the 6th BlackboxNLP Workshop: Analyzing and Interpreting Neural Networks for NLP 
- Authors: Yonatan Belinkov | Sophie Hao | Jaap Jumelet | Najoung Kim | Arya McCarthy | Hosein Mohebbi 
- Summary: 
    1. 일관성이 기대되는 태스크 (예: 질문 답변, 설명 등)에 대해서도 다양한 맥락에서 일관된 답변을 주지 않는 대규모 언어 모델 (LLM)은 문제가 있습니다. 이 논문은 두 개 이상의 답이 올바른 경우의 미사양 상황에서 자기 일관성을 평가하기 위한 평가 벤치마크를 제시합니다.
    2. OpenAI 모델 스위트에서 애매한 정수 수열 완성 작업에 대한 일련의 행동 실험을 실시하였으며, 모델의 일관성 평균은 67%에서 82%로 나타났으며, 모델의 일관성이 무작위인 경우보다 훨씬 높았으며, 모델 능력이 향상됨에 따라 증가합니다.
    3. 또한, 모델은 화자 변경 및 시퀀스 길이 변경을 포함한 일련의 견고성 검사를 통해 자기 일관성을 유지하는 경향을 보이며, 이 결과는 자기 일관성이 특별히 그런 것이 아니라 신회 기능으로 나타난다는 것을 나타냅니다.

###### Character-Level Chinese Backpack Language Models (https://aclanthology.org/2023.blackboxnlp-1.8/)
- Anthology ID: 2023.blackboxnlp-1.8 
- Volume: Proceedings of the 6th BlackboxNLP Workshop: Analyzing and Interpreting Neural Networks for NLP 
- Authors: Yonatan Belinkov | Sophie Hao | Jaap Jumelet | Najoung Kim | Arya McCarthy | Hosein Mohebbi 
- Summary: 
    1. "Backpack"은 영어 언어 모델에 대한 해석 가능성을 높이기 위해 예측을 토큰 센스 구성 요소의 가중합으로 분해하는 Transformer 대안이다. 그러나 Backpack은 토큰으로 정의된 의미에 의존하기 때문에 영어 이외의 언어에서 얼마나 유용할 수 있는지 의문이 제기된다.
    2. 이 논문에서는 중국어에서 Character로 토큰화된 Backpack 언어 모델을 훈련하고 평가하여 이의 의미를 해석하고 제어한다. 결과적으로 중국어 Backpack 언어 모델은 Transformer와 비교하여 우수한 성능을 발휘하며, 문자 수준에서 풍부한 의미를 학습하는 것을 확인하였다.
    3. 이 연구는 Backpack의 인과관계를 특정 character sense로 정확히 지역화하고 편향성을 감소시키는 것과 같은 해석 가능성을 통해 모델을 제어함으로써 효과적으로 문제를 해결할 수 있다는 것을 보여준다.

###### Unveiling Multilinguality in Transformer Models: Exploring Language Specificity in Feed-Forward Networks (https://aclanthology.org/2023.blackboxnlp-1.9/)
- Anthology ID: 2023.blackboxnlp-1.9 
- Volume: Proceedings of the 6th BlackboxNLP Workshop: Analyzing and Interpreting Neural Networks for NLP 
- Authors: Yonatan Belinkov | Sophie Hao | Jaap Jumelet | Najoung Kim | Arya McCarthy | Hosein Mohebbi 
- Summary: 
    1. 최근 연구에 따르면 Transformers의 feed-forward 모듈은 입력에서 특정 패턴을 캡처하는 키-밸류 메모리의 모음으로 볼 수 있다고 한다. 이로 인해 예측과정이 점진적으로 이뤄지며 출력층에 가까워질수록 최종 토큰 선택에 수렴한다. 이러한 흥미로운 관점은 다국어 모델이 이 메커니즘을 어떻게 활용할 수 있는지에 대한 질문을 제기한다. 
    2. 특히, 두 개 이상의 언어로 훈련된 오토리그레시브 모델의 경우, 모든 뉴런 (레이어 전체)이 모든 언어에 동일하게 반응합니까? 답변은 아니다! 저자들의 가설은 pre-training 단계에서 특정 모델 매개변수가 강한 언어별 특징을 학습하고 다른 값은 언어에 구애받지 않는 특징을 학습한다는 것이다. 
    3. 이 가설을 검증하기 위해, 초기 pre-training에서 사용한 두 언어의 병렬 코퍼스를 활용한 실험을 수행하였다. 결과는 네트워크의 입력 또는 출력에 가까운 레이어는 중간 레이어보다 언어별 특징을 더 많이 나타내는 것을 보여주었다.

###### Why Bother with Geometry? On the Relevance of Linear Decompositions of Transformer Embeddings (https://aclanthology.org/2023.blackboxnlp-1.10/)
- Anthology ID: 2023.blackboxnlp-1.10 
- Volume: Proceedings of the 6th BlackboxNLP Workshop: Analyzing and Interpreting Neural Networks for NLP 
- Authors: Yonatan Belinkov | Sophie Hao | Jaap Jumelet | Najoung Kim | Arya McCarthy | Hosein Mohebbi 
- Summary: 
    1. Transformer 임베딩은 일련의 인수들의 합으로 선형 분해될 수 있으며, 이러한 분해와 네트워크 입력 또는 구성 요소와의 관계가 잘 정의되어 있는 것으로 알려져 있지만, 이러한 수학적 재정의가 실증적으로 의미 있는지에 대한 연구는 아직 불충분한 상태이다.
    2. 본 연구에서는 기계 번역에 사용되는 Transformer 디코더의 표현을 두 가지 임베딩 분해 방법을 사용하여 연구하였다.
    3. 결과적으로, 분해로 얻은 지표들은 모델 성능과 효과적으로 상관관계를 가지지만, 여러 실행 간의 변동성은 이러한 문제에 대한 보다 미묘한 접근이 필요하다는 것을 시사한다.

###### Investigating Semantic Subspaces of Transformer Sentence Embeddings through Linear Structural Probing (https://aclanthology.org/2023.blackboxnlp-1.11/)
- Anthology ID: 2023.blackboxnlp-1.11 
- Volume: Proceedings of the 6th BlackboxNLP Workshop: Analyzing and Interpreting Neural Networks for NLP 
- Authors: Yonatan Belinkov | Sophie Hao | Jaap Jumelet | Najoung Kim | Arya McCarthy | Hosein Mohebbi 
- Summary: 
    1. Transformer 기반 언어 모델의 각 레이어가 어떤 언어 정보를 인코딩하는지는 NLP 커뮤니티에 매우 흥미로운 문제이다. 그러나 기존 연구는 주로 단어 수준의 표현과 마스크 토큰 학습 목적을 가진 encoder-only 언어 모델에 초점을 맞추었다. 
    2. 이 연구에서는 semantic structural probing라는 방법을 소개하여 문장 수준 표현을 연구하는데 사용하였다. 우리의 방법을 encoder-only, decoder-only, encoder-decoder 모델과 다양한 크기의 언어 모델에 적용하였고, 두 가지 작업인 의미적 텍스트 유사성과 자연어 추론의 문맥에서 모델 패밀리는 성능과 레이어의 동적을 많이 다르게 나타냈다. 
    3. 하지만 결과는 모델의 크기에 대한 영향이 크게 없음을 발견하였다.

###### Causal Abstraction for Chain-of-Thought Reasoning in Arithmetic Word Problems (https://aclanthology.org/2023.blackboxnlp-1.12/)
- Anthology ID: 2023.blackboxnlp-1.12 
- Volume: Proceedings of the 6th BlackboxNLP Workshop: Analyzing and Interpreting Neural Networks for NLP 
- Authors: Yonatan Belinkov | Sophie Hao | Jaap Jumelet | Najoung Kim | Arya McCarthy | Hosein Mohebbi 
- Summary: 
    1. 최근 연구는 큰 언어 모델이 최종 답변 이전에 중간 추론 단계, 즉 chain of thought (CoT)를 생성하게 되면 다단계 추론 과제에서 더 높은 정확성을 보인다고 제안하고 있다. 하지만 CoT가 어떻게 언어 모델의 정확성을 향상시키는지, 특히 언어 모델이 CoT를 사용하여 최종 답변을 도출하는지에 대해서는 여전히 불분명하다.
    2. 본 논문은 산술 문제를 다루며 이 질문에 대한 답을 찾기 위해 (i) 언어 모델의 CoT의 정확성을 평가하고, (ii) 인과 추상화를 사용하여 CoT에 따라 생성된 중간 토큰이 언어 모델의 최종 답변에 인과적인 영향을 미치는지를 평가한다.
    3. 연구 결과, CoT-prompted된 언어 모델들은 산술 문제에서 정확한 답변과 CoT 사이에 매우 높은 상관관계를 가지며, 정확한 CoT를 생성할 때는 CoT에 기술된 추론과 거의 유사한 인과 모델을 실현한다. 이러한 실현 정도가 산술 문제 전체의 정확성과 더불어 관련이 있는 것으로 나타났다. 이러한 결과는 일부 CoT-prompted된 언어 모델들이 최종 답변을 도출하기 위해 CoT를 사용하여 다단계 산술 추론에서 더 좋은 결과를 얻을 수 있다는 것을 시사한다. 하지만 어떤 언어 모델들은 다른 내부 과정도 관여할 수 있다.

###### Enhancing Interpretability Using Human Similarity Judgements to Prune Word Embeddings (https://aclanthology.org/2023.blackboxnlp-1.13/)
- Anthology ID: 2023.blackboxnlp-1.13 
- Volume: Proceedings of the 6th BlackboxNLP Workshop: Analyzing and Interpreting Neural Networks for NLP 
- Authors: Yonatan Belinkov | Sophie Hao | Jaap Jumelet | Najoung Kim | Arya McCarthy | Hosein Mohebbi 
- Summary: 
    1. NLP의 해석 가능성 메서드는 특정 시스템 아키텍처의 의미론에 대한 통찰력을 제공하기 위해 개발되었다. 이 논문에서는 단어 임베딩에 초점을 맞춘 감독 학습 방법을 제안하고, 주어진 도메인(예: 스포츠, 직업)에서 사람의 유사성 판단 예측에 크게 기여하는 모델 특징의 하위 집합을 식별한다.
    2. 이 방법은 8개의 독립적인 의미적 도메인에서 원래의 임베딩의 20-40%만 유지하며, 도메인마다 다른 특징 집합을 유지하는 것을 보여준다.
    3. 이 논문에서는 유지된 특징들의 의미론을 해석하기 위해 두 가지 접근 방식을 제시한다. 첫 번째는 유지된 임베딩의 첫 번째 주성분에 도메인 단어들의 점수를 얻고, 이 점수 프로필을 따라 가장 관련 있는 용어를 추출한다. 이 분석은 인간들이 예를 들어 스포츠를 성별 포함성과 국제성에 따라 구분한다는 것을 드러낸다. 두 번째 접근 방식은 유지된 특징들을 535개의 단어 데이터셋에 대해 65개 의미적으로 주석이 달린 차원을 예측하는 조사 작업의 변수로 사용한다. 직업에 유지된 특징이 인지, 감정 및 사회적 차원을 가장 잘 예측하는 반면, 과일 또는 야채에 유지된 특징은 맛 차원을 가장 잘 예측한다. 인공지능 시스템과 인간의 지식 간의 조화에 대한 함의에 대해 논의한다.

###### When Your Language Model Cannot Even Do Determiners Right: Probing for Anti-Presuppositions and the Maximize Presupposition! Principle (https://aclanthology.org/2023.blackboxnlp-1.14/)
- Anthology ID: 2023.blackboxnlp-1.14 
- Volume: Proceedings of the 6th BlackboxNLP Workshop: Analyzing and Interpreting Neural Networks for NLP 
- Authors: Yonatan Belinkov | Sophie Hao | Jaap Jumelet | Najoung Kim | Arya McCarthy | Hosein Mohebbi 
- Summary: 
    1. 대형 언어 모델(Large Language Model, LLM)들의 언어 능력을 조사하는 연구에서, 선결 가정(presupposition) 현상과 그 원리인 Maximize Presupposition! (MP!)의 연구가 아직 이루어지지 않은 현상을 연구하였다.
    2. 실험을 통해 언어 모델이 다양한 반선결 가정(anti-presupposition)을 어떻게 다루고, MP! 원리를 적용하는지 조사하였다.
    3. 연구 결과, LLM은 MP! 원리를 따르는 것보다 문맥 기반 n-gram을 복제하는 경향이 있으며, 자연어 추론 데이터로 세밀한 조정을 해도 MP! 원리에 충실하지 않음을 발견하였다. 특히, 상대적으로 간단한 언어적 맥락에서 LLM은 한정사를 올바르게 예측하는 데 어려움을 겪는 것으로 나타났다.

###### Introducing VULCAN: A Visualization Tool for Understanding Our Models and Data by Example (https://aclanthology.org/2023.blackboxnlp-1.15/)
- Anthology ID: 2023.blackboxnlp-1.15 
- Volume: Proceedings of the 6th BlackboxNLP Workshop: Analyzing and Interpreting Neural Networks for NLP 
- Authors: Yonatan Belinkov | Sophie Hao | Jaap Jumelet | Najoung Kim | Arya McCarthy | Hosein Mohebbi 
- Summary: 
    1. 예시는 복잡한 개념과 연결을 이해하는 데 도움이 되는 강력한 도구이다. 본 논문에서는 문자열, 그래프, 트리, 정렬, 어텐션 등을 시각화해주는 visualization tool인 VULCAN에 대해 설명한다. VULCAN은 언어적 구조와 신경망 모델의 속성을 동시에 시각화할 수 있는 독특한 능력을 가지고 있어 신경 기호주의 모델(neuro-symbolic models)에 매우 적합하다. 
    
    2. 신경 기호주의 모델은 신경망을 언어적으로 기반된 구조와 결합하여 순전히 신경망 기반의 블랙박스 end-to-end 모델의 해석 가능성을 높이는 것을 목표로 한다. VULCAN은 이러한 해석 가능성을 실용적으로 돕기 위해 설계되었으며, 사용하기 쉽고 강력한 기능을 제공한다. 
    
    3. VULCAN은 open-source 소프트웨어로, 조사자들이 신경 기호주의 모델의 시각화를 통해 복잡한 개념과 모델의 동작을 더 잘 이해할 수 있도록 돕는다.

###### The Self-Contained Negation Test Set (https://aclanthology.org/2023.blackboxnlp-1.16/)
- Anthology ID: 2023.blackboxnlp-1.16 
- Volume: Proceedings of the 6th BlackboxNLP Workshop: Analyzing and Interpreting Neural Networks for NLP 
- Authors: Yonatan Belinkov | Sophie Hao | Jaap Jumelet | Najoung Kim | Arya McCarthy | Hosein Mohebbi 
- Summary: 
    1. 최근에는 사전 학습 언어 모델(PLM)이 부정을 해석하는 능력을 평가하기 위한 다양한 방법이 제안되었다. 
    2. 이 논문에서는 Gubelmann and Handschuh (2022) 연구를 기반으로 하여 PLM의 예측이 입력의 극성에 따라 어떻게 변경되는지를 연구했다.
    3. 개선된 버전인 Self-Contained Neg Test를 소개하여 roberta와 bert 모델의 부정에 대한 민감도를 평가했고, roberta의 결과가 기대에 부합하는 반면 bert는 부정에 거의 민감하지 않음을 보여주었다.

###### Investigating the Effect of Discourse Connectives on Transformer Surprisal: Language Models Understand Connectives, Even So They Are Surprised (https://aclanthology.org/2023.blackboxnlp-1.17/)
- Anthology ID: 2023.blackboxnlp-1.17 
- Volume: Proceedings of the 6th BlackboxNLP Workshop: Analyzing and Interpreting Neural Networks for NLP 
- Authors: Yonatan Belinkov | Sophie Hao | Jaap Jumelet | Najoung Kim | Arya McCarthy | Hosein Mohebbi 
- Summary: 
    1. 최근의 neural language models(NLMs)은 Transformer를 기반으로 한다. 이 연구에서는 discourse connectives가 NLMs에 미치는 영향과 Transformer Surprisal 점수에 대해 연구하였다.
    2. 큰 규모의 NLMs는 감탄점 connective가 사용될 때 인간의 행동 데이터와 유사한 패턴을 보였다. 그러나 대조적인 connective의 경우 connective와 관련된 효과가 사라지는 경향을 보였다.
    3. 연장된 데이터셋을 사용하여 GPT-Neo로 결과를 검증한 결과, 대체로 일관된 패턴을 확인할 수 있다.

###### METAPROBE: A Representation- and Task-Agnostic Probe (https://aclanthology.org/2023.blackboxnlp-1.18/)
- Anthology ID: 2023.blackboxnlp-1.18 
- Volume: Proceedings of the 6th BlackboxNLP Workshop: Analyzing and Interpreting Neural Networks for NLP 
- Authors: Yonatan Belinkov | Sophie Hao | Jaap Jumelet | Najoung Kim | Arya McCarthy | Hosein Mohebbi 
- Summary: 
    1. "컨텍스처에 의존한 표현을 조사하는 것은 과제 특정 모델 예측을 근거로하는 언어적인 레이블의 진실과의 비교를 포함한다. 그러나 이 방법은 어떤 정보가 분류기에 의해 복구될 수 있는지를 보여주지만, 분류기가 표현을 활용하여 결정을 내리는 방식은 보여주지 않는다."
    2. "우리는 이후의 문제를 해결하기 위해 다음을 묻습니다: 과제 분류자들은 임베딩 공간에서 표현 및 과제에 독립적인 기하학적 패턴에 의존합니까?"
    3. "우리는 MetaProbe라는 접근법을 개발하여 표현의 기하학적 특성을 사용하여 과제 특정 분류자의 동작을 예측합니다. 우리의 실험은 분류자의 예측을 예측할 수 있는 표현 간의 보편적인 기하학적 패턴의 존재를 보여줍니다. 결과적으로, 우리는 컨텍스트에 의존하는 표현의 놀라운 성능에 대한 기하학적 설명을 제시할 수 있습니다."

###### How Much Consistency Is Your Accuracy Worth? (https://aclanthology.org/2023.blackboxnlp-1.19/)
- Anthology ID: 2023.blackboxnlp-1.19 
- Volume: Proceedings of the 6th BlackboxNLP Workshop: Analyzing and Interpreting Neural Networks for NLP 
- Authors: Yonatan Belinkov | Sophie Hao | Jaap Jumelet | Najoung Kim | Arya McCarthy | Hosein Mohebbi 
- Summary: 
    1. "Contrast set consistency"는 모델이 같은 지식을 기반으로 한 번 더미처럼 비슷하지만 최소한의 차이를 가진 예시에 올바른 응답을 정확하게 준다는 것을 평가하는 robustness 측정이다.
    2. 상대 일관성(relative consistency)과 일관성을 보완하여 모델의 일관성을 평가한다. 100% 상대 일관성을 가진 모델은 정확성에 대한 일관성이 최고점에 도달한 것이다. 
    3. 우리는 일관성에 대한 이전 연구들을 검토하고, 상대 일관성이 다른 모델과 비교했을 때 모델의 일관성 평가를 변경할 수 있다는 사실을 관찰한다. 우리의 제안된 측정법과 관련된 통찰력이 모델의 일관된 행동을 촉진하는 미래의 연구에 영향을 미칠 것으로 예상된다.

###### Investigating the Encoding of Words in BERT’s Neurons Using Feature Textualization (https://aclanthology.org/2023.blackboxnlp-1.20/)
- Anthology ID: 2023.blackboxnlp-1.20 
- Volume: Proceedings of the 6th BlackboxNLP Workshop: Analyzing and Interpreting Neural Networks for NLP 
- Authors: Yonatan Belinkov | Sophie Hao | Jaap Jumelet | Najoung Kim | Arya McCarthy | Hosein Mohebbi 
- Summary: 
    1. 사전훈련 언어 모델(PLMs)은 대부분의 최신 NLP 기술의 기반이지만, 이들은 본질적으로 블랙박스이다. 특히 개별 뉴런에서 표현되는 지식을 사람들은 명확하게 이해하지 못한다. 
    2. 우리는 시각 모델의 뉴런들에 대한 분해적 해석 기법인 특징 시각화를 영감으로 하여 Activation maximization을 NLP에 적용한 첫 대규모 시도에 대해 조심스러운 사례를 제시한다. 
    3. 우리는 BERT 모델에 feature textualization 기술을 적용하여 개별 뉴런에 인코딩된 지식을 해석하고 상징화할 수 있는지 조사한다. 우리는 개별 뉴런이 단어와 같은 명확한 기호적 단위를 표현하지 않는다는 것을 발견했다.

###### Evaluating Transformer’s Ability to Learn Mildly Context-Sensitive Languages (https://aclanthology.org/2023.blackboxnlp-1.21/)
- Anthology ID: 2023.blackboxnlp-1.21 
- Volume: Proceedings of the 6th BlackboxNLP Workshop: Analyzing and Interpreting Neural Networks for NLP 
- Authors: Yonatan Belinkov | Sophie Hao | Jaap Jumelet | Najoung Kim | Arya McCarthy | Hosein Mohebbi 
- Summary: 
    1. 최근 연구들은 Transformer 모형이 일부 정규 및 문맥 자유 언어조차도 학습하는 데 제한되어 있다는 이론적 한계가 있다고 주장하고 있다.
    2. 이 논문에서는 Transformer의 얕게 문맥에 의존적인 언어를 학습하는 능력을 실험하고, 기존의 LSTM보다는 성능이 좋지 않음을 발견하였다.
    3. 학습된 self-attention 패턴과 표현은 종속성 관계를 모델링하고, 카운팅 동작을 보여주었으며, 이는 모형이 언어를 해결하는 데 도움이 되었을 것이다.

###### Layered Bias: Interpreting Bias in Pretrained Large Language Models (https://aclanthology.org/2023.blackboxnlp-1.22/)
- Anthology ID: 2023.blackboxnlp-1.22 
- Volume: Proceedings of the 6th BlackboxNLP Workshop: Analyzing and Interpreting Neural Networks for NLP 
- Authors: Yonatan Belinkov | Sophie Hao | Jaap Jumelet | Najoung Kim | Arya McCarthy | Hosein Mohebbi 
- Summary: 
    1. GPT와 PALM과 같은 대형 언어 모델은 텍스트 생성, 질문 답변, 번역과 같은 다양한 NLP 태스크에서 뛰어난 성과를 보이고 있지만, 사회적 편향이 내재되어 있다.
    2. 최근 연구에서는 반복적인 공간 투영 (INLP) 및 대조적 데이터 증강 (CDA)과 같은 편향 제거 기법을 제안하고 있다. 또한, 이러한 모델의 복잡성을 이해하는 데 관심이 증가하고 있다.
    3. 본 연구에서는 OPT, LLaMA 및 LLaMA2와 그들의 편향 제거된 버전을 사용하여 최신 LLM을 성능 평가하고, Logit Lens라는 방법을 사용하여 편향과 다른 transformer 레이어와의 관계를 조사한다.

###### Not Wacky vs. Definitely Wacky: A Study of Scalar Adverbs in Pretrained Language Models (https://aclanthology.org/2023.blackboxnlp-1.23/)
- Anthology ID: 2023.blackboxnlp-1.23 
- Volume: Proceedings of the 6th BlackboxNLP Workshop: Analyzing and Interpreting Neural Networks for NLP 
- Authors: Yonatan Belinkov | Sophie Hao | Jaap Jumelet | Najoung Kim | Arya McCarthy | Hosein Mohebbi 
- Summary: 
    1. 단어의 의미를 표현하는 벡터 공간 모델들은 비슷한 문맥에서 등장하는 단어들은 비슷한 의미를 가진다고 가정하는데, 이러한 모델들은 논리적 추론을 수행하는 NLP 응용 프로그램에 어려움이 있다. 
    2. 그러나 사전 훈련된 언어 모델들(BERT, RoBERTa, GPT-2, GPT-3)의 논리적 작업 성능에 대한 보고는 상반된다. 
    3. 본 연구에서는 BERT, RoBERTa, GPT-2, GPT-3 모델들이 scalar adverbs라고 불리는 단어들을 얼마나 알고 있는지 조사하였고, 논리적 의미의 어떤 측면을 포착하고 있는지 알 수 있었다.

###### Rigorously Assessing Natural Language Explanations of Neurons (https://aclanthology.org/2023.blackboxnlp-1.24/)
- Anthology ID: 2023.blackboxnlp-1.24 
- Volume: Proceedings of the 6th BlackboxNLP Workshop: Analyzing and Interpreting Neural Networks for NLP 
- Authors: Yonatan Belinkov | Sophie Hao | Jaap Jumelet | Najoung Kim | Arya McCarthy | Hosein Mohebbi 
- Summary: 
    1. 자연어는 대량의 언어 모델이 정보를 처리하고 저장하는 방식을 설명하는 데 유용한 매체입니다. 그러나 이러한 설명의 충실성을 평가하는 것은 도전적입니다.
    2. 우리는 자연어 설명이 특정 개념을 나타내는 텍스트 입력에 대한 한 뉴런이 표현한다고 주장하는 것을 평가하기 위해 두 가지 평가 모드를 개발합니다.
    3. 우리의 프레임워크를 GPT-4로 생성된 GPT-2 XL 뉴런의 설명에 적용하여, 가장 확신하는 설명도 오류율이 높고 인과 효능이 전혀 없음을 보여줍니다. 이 논문은 자연어가 설명에 적합한 선택인지, 그리고 뉴런이 최상의 분석 레벨인지에 대해 비판적으로 평가를 마칩니다.

###### NPIs Aren’t Exactly Easy: Variation in Licensing across Large Language Models (https://aclanthology.org/2023.blackboxnlp-1.25/)
- Anthology ID: 2023.blackboxnlp-1.25 
- Volume: Proceedings of the 6th BlackboxNLP Workshop: Analyzing and Interpreting Neural Networks for NLP 
- Authors: Yonatan Belinkov | Sophie Hao | Jaap Jumelet | Najoung Kim | Arya McCarthy | Hosein Mohebbi 
- Summary: 
    1. 큰 규모 언어 모델에서 부정적 극성 항목(NPIs)의 허가에 대해 조사하여, 모델이 NPI를 어떻게 학습하고, 구문-의미 인터페이스에서 언어 현상으로써 이해하는지에 대한 풍부한 그림을 제시합니다.
    2. NPI는 특정 허가 문맥에서만 나타나는 단어들로, 부정이 전형적인 예입니다. 기존의 많은 연구들과는 달리, 우리는 NPI와 그 허가 환경이 통합된 클래스가 아니라 다양한 환경에서 가능한 다른 NPI를 고려합니다.
    3. 다양한 모델에서 이 현상을 연구함으로써, 모델 아키텍처의 특징, 훈련 데이터의 속성 및 NPI 현상의 언어적 특성이 성능에 어떤 영향을 미칠지 탐구할 수 있습니다.

###### Memory Injections: Correcting Multi-Hop Reasoning Failures During Inference in Transformer-Based Language Models (https://aclanthology.org/2023.blackboxnlp-1.26/)
- Anthology ID: 2023.blackboxnlp-1.26 
- Volume: Proceedings of the 6th BlackboxNLP Workshop: Analyzing and Interpreting Neural Networks for NLP 
- Authors: Yonatan Belinkov | Sophie Hao | Jaap Jumelet | Najoung Kim | Arya McCarthy | Hosein Mohebbi 
- Summary: 
    1. "다중 단계 추론 질문에 대답하기 위해서는 다양한 정보를 알맞게 찾아내고 종합해야 한다. 하지만 대규모 언어 모델은 이러한 추론을 일관되게 수행하기 어렵다는 문제가 있다. 
    2. 우리는 대상 언어 모델의 특정 어텐션 헤드에 주요 정보를 집중적으로 주입하여 다중 단계 추론 실패를 정확하게 찾아내고 수정하는 방식을 제시한다.
    3. 우선, GPT-2 모델의 layer별 활성화를 분석하여 단일 단계와 다중 단계의 입력에 대한 모델의 반응을 알아봤다. 그리고 우리는 추론 시 특정 위치에서 관련된 정보를 주입하는 "기억"을 사용자가 삽입할 수 있는 메커니즘을 제안한다. 이렇게 모델이 추론 중에 추가적인 관련 정보를 통합하도록 하는 것으로 다중 단계 입력의 품질을 향상시킨다. 실험적으로, 핵심 어텐션 레이어에 간단하고 효율적인 타겟된 메모리 주입은 다중 단계 작업에서 원하는 다음 토큰의 확률을 최대 424% 증가시킬 수 있다."

###### Systematic Generalization by Finetuning? Analyzing Pretrained Language Models Using Constituency Tests (https://aclanthology.org/2023.blackboxnlp-1.27/)
- Anthology ID: 2023.blackboxnlp-1.27 
- Volume: Proceedings of the 6th BlackboxNLP Workshop: Analyzing and Interpreting Neural Networks for NLP 
- Authors: Yonatan Belinkov | Sophie Hao | Jaap Jumelet | Najoung Kim | Arya McCarthy | Hosein Mohebbi 
- Summary: 
    1. 구성성 테스트(replicate constituency tests)를 수행하는데 성능이 충분하지 않음을 보여주는 선행 pre-trained sequence-to-sequence language models(BART, T5)의 fine-tuning 설정이 다르게 되어 있으면 어떤 영향을 미치는지 조사하였다.
    2. fine-tuning 도중 구성성 테스트와 문장 유형의 다양한 조합을 사용하여 평가 설정을 설계하였는데, 모델은 fine-tuning 중에 보았던 특정 유형의 문장에서만 언어 변환을 재현할 수 있으며, 다른 설정에서는 성능이 크게 저하되어 체계적인 일반화 부족을 보여준다.
    3. 이러한 결과는 모델이 종종 문장을 표면적인 수준에서 구성 성분 수준의 문법 구조와 관련이 없도록 학습하는 것을 보여준다. 이러한 결과는 pre-trained language models가 downstream tasks에서 취약성을 보이는 이유의 일부를 설명할 수 있다.

###### On Quick Kisses and How to Make Them Count: A Study on Event Construal in Light Verb Constructions with BERT (https://aclanthology.org/2023.blackboxnlp-1.28/)
- Anthology ID: 2023.blackboxnlp-1.28 
- Volume: Proceedings of the 6th BlackboxNLP Workshop: Analyzing and Interpreting Neural Networks for NLP 
- Authors: Yonatan Belinkov | Sophie Hao | Jaap Jumelet | Najoung Kim | Arya McCarthy | Hosein Mohebbi 
- Summary: 
    1. "Psycholinguistic 연구들은 사건을 인지하는데 있어 우리의 정신적 인식은 단순히 그것을 설명하는데 사용된 어휘뿐 아니라 구문 구조에도 의존한다고 제안했습니다." 
    2. 우리는 BERT를 사용하여 문법 구조가 사건 지속 시간과 유사성에 미치는 영향을 조사하기 위해 영어 자극을 사용한 두 가지 실험을 제시합니다. 
    3. 우리는 i) BERT 벡터 차원이 인간의 결과와 일치하여 count 구문에서 점동 및 지속적인 사건에 대해 더 짧은 지속 시간을 인코딩한다는 것을 보여주었으며, 반대로 ii) BERT 의미 유사성은 count 구문에서 지속적인 사건이 향해야 하는 개념적 변화를 포착하지 못한다는 것을 발견했습니다.

###### Identifying and Adapting Transformer-Components Responsible for Gender Bias in an English Language Model (https://aclanthology.org/2023.blackboxnlp-1.29/)
- Anthology ID: 2023.blackboxnlp-1.29 
- Volume: Proceedings of the 6th BlackboxNLP Workshop: Analyzing and Interpreting Neural Networks for NLP 
- Authors: Yonatan Belinkov | Sophie Hao | Jaap Jumelet | Najoung Kim | Arya McCarthy | Hosein Mohebbi 
- Summary: 
    1. 언어 모델(LMs)은 훈련 데이터에서 학습한 성별 편향을 포함하여 다양한 유형의 원치 않는 편향을 보여주고 강조한다. 그러나 우리는 일반적인 언어 모델링 성능에 손상을 주지 않고 이러한 행동을 효과적으로 변경할 수 있는 도구가 부족하다. 
    2. 이 논문에서는 인과 관계를 찾기 위한 세 가지 방법을 고찰하는데, 인과 매개 분석, 자동 회로 탐색 및 Differential Masking을 기반으로 한 DiffMask+라는 새로운 효율적인 방법을 사용한다. 
    3. GPT-2 작은 모델과 성별 편향 문제에 이러한 방법을 적용하고, 발견한 구성요소를 사용하여 편향 완화를 위한 매개 효율적인 fine-tuning을 수행한다. 그 결과, 방법의 계산 요구 사항에 큰 차이가 있음에도 불구하고 특정 부분에서 중복이 크게 나타나며, 일반 언어 모델링에 대한 손상이 덜한 상태로 성별 편향을 완화하는데 성공한다.

