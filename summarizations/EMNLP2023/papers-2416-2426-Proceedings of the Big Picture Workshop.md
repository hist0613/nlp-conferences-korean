# Korean Three-Line Summarizations of Papers 2416-2426 in Proceedings of the Big Picture Workshop
###### Proceedings of the Big Picture Workshop (https://aclanthology.org/2023.bigpicture-1.0/)
- Anthology ID: 2023.bigpicture-1.0 
- Volume: Proceedings of the Big Picture Workshop 
- Summary: 
    요약문을 생성할 수 없습니다.

###### Where are We in Event-centric Emotion Analysis? Bridging Emotion Role Labeling and Appraisal-based Approaches (https://aclanthology.org/2023.bigpicture-1.1/)
- Anthology ID: 2023.bigpicture-1.1 
- Volume: Proceedings of the Big Picture Workshop 
- Summary: 
    1. 텍스트의 감정 분석은 컴퓨터가 감정을 이해할 수 있도록 하는 다양한 자연어 처리 작업을 포함한다. 
    2. 이 논문에서는 감정 역할 지정 (emotion role labeling)과 이벤트 중심 감정 분류 (event-focused emotion classification)이 분리되어 연구되어왔다고 주장하며, 두 관점을 상황에 맞게 제안하고 연구 질문을 논의한다. 
    3. 감정은 이벤트에 의해 발생하고 가리키는데, 이 논문은 자연어 처리에 감정 역할 지정 시 이벤트 개념을 도입하는 것을 제안한다.

###### Working Towards Digital Documentation of Uralic Languages With Open-Source Tools and Modern NLP Methods (https://aclanthology.org/2023.bigpicture-1.2/)
- Anthology ID: 2023.bigpicture-1.2 
- Volume: Proceedings of the Big Picture Workshop 
- Summary: 
    1. 우리는 특히 우랄어에 초점을 맞춘 멸종 언어 문서화를 위한 인프라 구축에 대한 연구를 제시한다. 
    2. 우리의 인프라는 항목이 XML 형식으로 구조화된 사전을 작성할 수 있는 도구들로 이루어져 있다. 
    3. 우리는 룰과 어휘를 통해 훈련 데이터를 생성하여 최신 최첨단 신경망 모델을 사용하여 이러한 사전과 도구를 개선하는 작업을 적극적으로 수행한다.

###### Computational Narrative Understanding: A Big Picture Analysis (https://aclanthology.org/2023.bigpicture-1.3/)
- Anthology ID: 2023.bigpicture-1.3 
- Volume: Proceedings of the Big Picture Workshop 
- Summary: 
    1. 본 논문에서는 computational narrative understanding 분야의 주요 연구 목표를 개요로 제공한다. 
    2. 이 분야는 사람들의 행동을 설명하는 핵심 메커니즘으로서 이야기를 중점으로 하는 몇 가지 연구 도메인이 있다.
    3. 본 논문은 narrative의 요소를 개요로 제공하면서 narrative의 다중성, 시간적 패턴, 사회 문화적 schema 등 세 가지 주요 연구 동향을 소개한다.

###### The Case for Scalable, Data-Driven Theory: A Paradigm for Scientific Progress in NLP (https://aclanthology.org/2023.bigpicture-1.4/)
- Anthology ID: 2023.bigpicture-1.4 
- Volume: Proceedings of the Big Picture Workshop 
- Summary: 
    1. 이 논문은 언어 구조에 대한 확장 가능하고 데이터 기반의 이론 개발을 중심으로 한 NLP의 과학적 진보 패러다임을 제안한다.
    2. 말뭉치에서 주어진 관심 행동 현상의 철저한 어노테이션을 가능하게 하는 방식으로 데이터를 수집한 뒤, 머신 러닝을 사용하여 이러한 현상에 대한 설명적인 이론을 구축함으로써 인공 지능 시스템에 활용할 수 있는 기반 블록을 구성한다.
    3. 이 논문은 데이터 수집과 이론적 모델링에 대한 원칙을 제시하여 AI에서의 언어 동작에 대한 복잡성을 다루고, 앞으로의 과학적 진보에 도움이 될 수 있다.

###### Thesis Distillation: Investigating The Impact of Bias in NLP Models on Hate Speech Detection (https://aclanthology.org/2023.bigpicture-1.5/)
- Anthology ID: 2023.bigpicture-1.5 
- Volume: Proceedings of the Big Picture Workshop 
- Summary: 
    1. 나의 박사학위 논문에서는 NLP 모델의 편향이 혐오 발언 감지 작업에 미치는 영향을 설명 가능성, 공격적인 초상화 편향 및 공정성 측면에서 조사하였다. 
    2. 그리고 이 논문에서는 내 논문의 주요 결론과 이를 NLP 커뮤니티에서 어떻게 활용할 수 있는지에 대해 논의한다. 
    3. 마지막으로, NLP 모델의 편향을 측정하고 완화하는 현재의 한계를 효과적으로 극복하기 위해 사회과학을 공부에 접목시키는 것이 필요하다는 결론을 도출하였다.

###### Large Language Models as SocioTechnical Systems (https://aclanthology.org/2023.bigpicture-1.6/)
- Anthology ID: 2023.bigpicture-1.6 
- Volume: Proceedings of the Big Picture Workshop 
- Summary: 
    1. 대형 언어 모델(LLMs)의 기대는 이들이 작동하는 더 큰 사회 기술적 참조 프레임을 무시하였다. 이 논문에서는 LLM의 추상화와 공정성과 관련된 문제를 논의하고, Selbst et al.(2019)의 다섯 가지 문제 상황 - The Framing Trap, The Portability Trap, The Formalism Trap, The Ripple Effect Trap, The Solutionism Trap - 을 LLM의 맥락에서 논의한다.
    2. 이전 연구와 예시에서 얻은 교훈을 바탕으로 LLM이 빠지는 함정을 각각 논의하고, 사회 기술적 렌즈를 통해 LLM 실패 지점을 파악하는 방법을 제안한다.
    3. 이러한 논의는 사회 기술적 관점에서 LLM을 보는 넓은 시각을 제공하며, 우리의 권고사항은 다양한 기술 및 사회 이해관계자들 사이에서 책임을 명확히 하고 미래의 LLM 연구를 영감을 줄 수 있다고 기대한다.

###### Towards Low-resource Language Generation with Limited Supervision (https://aclanthology.org/2023.bigpicture-1.7/)
- Anthology ID: 2023.bigpicture-1.7 
- Volume: Proceedings of the Big Picture Workshop 
- Summary: 
    1. 다양한 자연어 생성(NLG) 작업에 대한 언어 기술을 저리자원 언어(LRL)에 적용하기 위한 연구 내러티브를 제시한다.
    2. 전 세계적으로 약 7,000개의 언어가 사용되는데 그 중 많은 언어들은 모델 학습을 위한 리소스가 부족하다.
    3. 이 연구에서는 전이 학습과 제한적 감독 기법을 효과적으로 활용하여 LRL에 대한 언어 기술을 가능하게 하는 세 가지 연구를 소개한다.

###### Transformers as Graph-to-Graph Models (https://aclanthology.org/2023.bigpicture-1.8/)
- Anthology ID: 2023.bigpicture-1.8 
- Volume: Proceedings of the Big Picture Workshop 
- Summary: 
    1. 이 논문은 Transformers가 본질적으로 그래프 간 모델이며, sequence가 그 중 특수한 경우라고 주장한다. Transformer의 attention weights는 그래프의 엣지와 기능적으로 동등하다.
    2. 그래프 간 Transformer 아키텍처는 이 능력을 명시적으로 표현하기 위해 그래프의 엣지를 attention 가중치 계산에 입력하고 attention과 유사한 함수로 그래프 엣지를 예측하며, 사전 학습된 Transformer가 학습한 잠재 그래프에 명시적 그래프를 통합한다.
    3. 반복적인 그래프 개선을 추가함으로써 입력, 출력 및 잠재 그래프의 공동 임베딩을 제공하며, 임의의 파이프 라인 또는 디코딩 전략 없이 완전한 그래프를 최적화하는 비자기 회귀 그래프 예측이 가능하다. 실험 결과는 이 아키텍처가 다양한 언어 구조를 모델링하는 데 최고 수준의 정확도를 달성한다는 것을 보여준다.

###### It’s MBR All the Way Down: Modern Generation Techniques Through the Lens of Minimum Bayes Risk (https://aclanthology.org/2023.bigpicture-1.9/)
- Anthology ID: 2023.bigpicture-1.9 
- Volume: Proceedings of the Big Picture Workshop 
- Summary: 
    1. MBR (Minimum Bayes Risk) 디코딩은 기계 학습 시스템의 출력을 확률적으로 가장 높은 것이 아니라 여러 후보 중에서 리스크(예상 오류)가 가장 낮은 출력으로 선택하는 방법이다.
    2. 우리는 최근의 몇 가지 방법들이 MBR의 특수한 경우로 쓰일 수 있음을 보여준다. 이를 통해 이러한 방법들의 성능에 대한 이론적인 근거를 제공하고 이전에는 경험적인 결과에 근거한 것으로만 보이던 몇 가지 결과를 설명한다. 
    3. 우리는 다양한 MBR 변형의 효과에 대한 이론적 및 경험적 결과를 제시하고, NLP 모델에 MBR을 적용하는데 대한 구체적인 권고사항과 이 분야에서의 미래 방향을 제안한다.

###### Analyzing Pre-trained and Fine-tuned Language Models (https://aclanthology.org/2023.bigpicture-1.10/)
- Anthology ID: 2023.bigpicture-1.10 
- Volume: Proceedings of the Big Picture Workshop 
- Summary: 
    1. 2018년 transformer 기반의 언어 모델이 소개된 이후, 현재의 NLP 모델들은 학문적인 벤치마크와 실제 응용 프로그램에서 놀라운 성능을 보여주고 있다. 그러나 이러한 진전은 대량의 텍스트로 사전 학습된 모델을 좁은 분야로 fine-tuning하는 단순하고 일반적인 파이프라인에 기반한 것이다.
    2. 이 논문에서는 pre-trained와 fine-tuned 된 언어 모델이 왜 잘 작동하는지, 어떤 언어 지식이 fine-tuning에 의해 어떻게 영향을 받는지에 대한 이해를 향상하기 위해 다양한 분석을 제시한다.
    3. 이 논문은 pre-trained와 fine-tuned 된 언어 모델의 능력과 이전에 설명되지 않았던 현상에 대한 새로운 통찰력을 제공하며, 모델의 일반화에 대한 적응 기술의 선택이 어떻게 영향을 미치는지에 대한 철저한 분석을 제공한다.

